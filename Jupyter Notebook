#Chapter 4: Applying machine learning models to predict disease groups (CD, UC, nonIBD) as the target variable (Y), using bacterial counts (X) as input features.

#4.1 Data preprocessing

#loading in libraries
    import numpy as np
    import pandas as pd

#list all the files under the input directory
    import os
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))

#Load in the two given datasets as dataframes and look at the first five rows
    genera_counts = pd.read_csv('/kaggle/input/project-lsa/genera.counts.tsv', sep = '\t')
    genera_counts.head()

    metadata = pd.read_csv('/kaggle/input/project-lsa/metadata.tsv', sep = '\t')
    metadata.head()

#Creating a new dataframe X that contains the genera_counts dataframe without the "Sample" column
    X = genera_counts.iloc[:,1:] 
    X.head()

#"Study.Group" is the only relevant column in the metadata dataframe.
#Select the column 'Study.Group' from the metadata dataset and create a series variable named 'Study_group'.
    Study_group = metadata['Study.Group']
    Study_group

#Looking if the three study groups are evenly distributed
    unique, counts = np.unique(Study_group, return_counts=True)
    value_counts = dict(zip(unique, counts))
    print(value_counts)

#Visualize the distribution above with a histogram
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.countplot(x=Study_group)
    plt.title('Distribution of disease groups')
    plt.xlabel('Study Group')
    plt.ylabel('Number of samples')
    plt.show()

#The distribution of the study groups is relatively balanced.
#There are slightly more samples in the 'CD' group in comparison with the two other groups. The number of samples in the 'UC' and 'nonIBD' groups are nearly identical.

#The scikit-learn library will be used to create our machine learning models, this requires all input variables to be in numeric format ['CD' = 0, 'UC' = 1, 'nonIBD' = 2].
    from sklearn.preprocessing import LabelEncoder
    encoder = LabelEncoder()
    y = encoder.fit_transform(Study_group)
    print(y)  
    print('mapping:', encoder.classes_)

#Split the data into training and validation data, for both features and target. The split is based on a random number generator.
#Supplying a numeric value to the random_state argument guarantees we get the same split every time we run the script.
    from sklearn.model_selection import train_test_split
    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 2)
    
#Get a summary of descriptive statistics for the dataset train_X
    train_X.describe()

#It can be seen in the table above that that there are columns where the maximum bacterial count equals zero.
#This means that the bacterium isn't detected in the samples present in the training dataframe.
#Therefore, these features will not provide additional information for the model so they will be deleted.
#The same columns deleted in the train dataset will be deleted in the test dataset.

#Identify the columns in train_X for which the sum of all the values in the column is zero
columns_to_remove = train_X.columns[(train_X.sum(axis=0) == 0)]

#Delete those colums from both the train_X and test_X datasets
train_X_filtered = train_X.drop(columns=columns_to_remove)
test_X_filtered = test_X.drop(columns=columns_to_remove)

print(f"Number of deleted columns: {len(columns_to_remove)}")
print(f'Percentage of deleted columns: {(len(columns_to_remove)/9694)*100 : .2f}%')

#Only 3.95% of the columns were deleted.

#Check the distribution of the different study groups in train_y
    unique, counts = np.unique(train_y, return_counts=True)
    value_counts = dict(zip(unique, counts))
    print(value_counts)

#Visualize the distribution above with a histogram
    sns.countplot(x=train_y)
    plt.title('Distribution of Study Groups in the Training Data')
    plt.xlabel('Study Group')
    plt.ylabel('Number of samples')
    plt.show()

#The distribution of the study groups is still relatively balanced in the same way as the original dataframe.

#4.2 Logistic regression

#Logistic regression requires standardised data.

#The distribution of ten random columns of the train_X data is visualised using boxplots to check if the above condition is met.
#Supplying a numeric value to the random_state argument guarantees we get the same ten random columns every time we run the script.
#To make the boxplots after standardisation, the same random state will be used. This way we can compare the unstandardised boxplots to the standardised boxplots.
    plt.figure(figsize=(18,10))
    plt.title('Distribution of Bacterial Counts Across Selected Bacteria', fontsize = 20)
    train_X_filtered.sample(10, axis="columns", random_state = 10).boxplot()
    plt.xticks(rotation=90)
    plt.show()

#Get a summary of descriptive statistics for the dataset train_X_filtered
    train_X_filtered.describe()

#The boxplot and table above show that the data has a wide range, with bacterial counts varying from zero to several thousand.
#Using StandardScaler to standardize the data, each feature will have a mean of 0 and a standard deviation of 1, making the data more uniform and suitable for machine learning algorithms that are sensitive to feature scales.
    from sklearn.preprocessing import StandardScaler
    scaler_std = StandardScaler()
    scaler_std.fit(train_X_filtered)
    print("Means:")
    print(scaler_std.mean_)
    print("Variances:")
    print(scaler_std.var_)

#The means and variances above will be used to transform the dataset.
    train_X_filtered_std = scaler_std.transform(train_X_filtered)
    train_X_filtered_std

#Transforming train_X_filtered_std into a dataframe
    train_X_filtered_std = pd.DataFrame(train_X_filtered_std, columns=train_X_filtered.columns)
    train_X_filtered_std

#Get a summary of descriptive statistics for the dataset train_X_filtered_std
    train_X_filtered_std.describe()

#In the table above, it can be seen that each feature now has a mean around 0 and a variance around 1.

#Check the distribution of the bacterial counts across the ten randomly selected bacteria after standardisation.
#The same random state as above is used to be able to compare both boxplot visualisations before and after standardisation.
    plt.figure(figsize=(18,10))
    plt.title('Distribution of Bacterial Counts Across Selected Bacteria After Standardisation', fontsize = 20)
    train_X_filtered_std.sample(10, axis="columns", random_state = 10).boxplot()
    plt.xticks(rotation=45, ha = 'right')
    plt.show()

#Apply standard scaling to the test dataset in the same way as done for the train dataset.
    test_X_filtered_std = pd.DataFrame(scaler_std.transform(test_X_filtered),columns=test_X_filtered.columns)
    test_X_filtered_std

#4.3 Model training
