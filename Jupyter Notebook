# Applying machine learning models to predict disease groups (CD, UC, nonIBD) as the target variable (Y), using bacterial counts (X) as input features.

#loading in libraries
    import numpy as np
    import pandas as pd

#list all the files under the input directory
    import os
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))

#Load in the two given datasets as dataframes and look at the first five rows
    genera_counts = pd.read_csv('/kaggle/input/project-lsa/genera.counts.tsv', sep = '\t')
    genera_counts.head()

    metadata = pd.read_csv('/kaggle/input/project-lsa/metadata.tsv', sep = '\t')
    metadata.head()

#Creating a new dataframe X that contains the genera_counts dataframe without the "Sample" column
    X = genera_counts.iloc[:,1:] 
    X.head()

#"Study.Group" is the only relevant column in the metadata dataframe.
#Select the column 'Study.Group' from the metadata dataset and create a series variable named 'Study_group'.
    Study_group = metadata['Study.Group']
    Study_group

#Looking if the three study groups are evenly distributed
    unique, counts = np.unique(Study_group, return_counts=True)
    value_counts = dict(zip(unique, counts))
    print(value_counts)

#Visualize the distributions with a histogram
    import seaborn as sns
    import matplotlib.pyplot as plt
    sns.countplot(x=Study_group)
    plt.title('Distribution of disease groups')
    plt.xlabel('Study Group')
    plt.ylabel('Number of samples')
    plt.show()

# The distributions across the groups are relatively balanced. There are slightly more samples in the 'CD' group in comparison with the two other groups. The number of samples in the 'UC' and 'nonIBD' groups are nearly identical.

#The scikit-learn library will be used to create our machine learning models, this requires all input variables to be in numeric format ['CD' = 0, 'UC' = 1, 'nonIBD' = 2].
    from sklearn.preprocessing import LabelEncoder
    
    encoder = LabelEncoder()
    y = encoder.fit_transform(Study_group)
    
    print(y)  
    print('mapping:', encoder.classes_)

#Split the data into training and validation data, for both features and target. The split is based on a random number generator.
#Supplying a numeric value to the random_state argument guarantees we get the same split every time we run the script.
    from sklearn.model_selection import train_test_split
    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 2)
    
#Get a summary of descriptive statistics for the dataset train_X
    train_X.describe()
# It can be seen in the table above that that there are columns where the maximum bacterial count equals zero.
# This means that the bacterium isn't detected in the samples present in the training dataframe.
# Therefore, these features will not provide additional information for the model so they will be deleted.
# The same columns deleted in the train dataset will be deleted in the test dataset.

#Identify the columns in train_X for which the sum of all the values in the column is zero
columns_to_remove = train_X.columns[(train_X.sum(axis=0) == 0)]

#Delete those colums from both the train_X and test_X datasets
train_X_filtered = train_X.drop(columns=columns_to_remove)
test_X_filtered = test_X.drop(columns=columns_to_remove)

print(f"Number of deleted columns: {len(columns_to_remove)}")
print(f'Percentage of deleted columns: {(len(columns_to_remove)/9694)*100 : .2f}%')
# Only 3.95% of the columns were deleted.

#Check the distribution in the different study groups in train_y





# Check which bacteria (features) rarely or don't appear in the samples (many zeros) and can be useful in determining whether certain features provide too little or no information for further analysis.
    stats_summary = pd.DataFrame({
        'mean': X.mean(),
        'median': X.median(),
        'std': X.std(),
        'percentage_zeros': (X == 0).mean() * 100})
    print(stats_summary.sort_values(by='percentage_zeros', ascending=False).head(50))  #View the top-50 features with the most zeros.
#The features with 100% zero values (13 columns) can be deleted because it will not help determine to which study group the sample belongs to.
# Identify columns with 100% zeros
    columns_to_remove = stats_summary[stats_summary['percentage_zeros'] == 100].index
# Remove these columns from the original dataset X
    X = X.drop(columns=columns_to_remove)
    X.describe()

# Split data into training and validation data, for both features and target. The split is based on a random number generator. Supplying a numeric value to the random_state argument guarantees we get the same split every time.
    from sklearn.model_selection import train_test_split
    train_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 1)

# Check the distribution in the different study groups in train_y.
    unique, counts = np.unique(train_y, return_counts=True)
    value_counts = dict(zip(unique, counts))
    print(value_counts)
# Visualize the distribution.
    sns.countplot(x=train_y)
    plt.title('Distribution of disease groups in the training data')
    plt.xlabel('Study Group')
    plt.ylabel('Number of samples')
    plt.show()
# The distributions across the groups are still relatively balanced.


# 1) Logistic regression model

